第一次正式打数据科学比赛，在这个比赛感谢各位大佬的分享，自己从中学到了很多东西。谢谢！！！
最终排名：（并不是前排，但自己也满足了）
初赛A榜：0.80518685 17/602 	
初赛B榜：0.77345468 16/602
复赛public榜：0.79632648 12/20
复赛private榜：0.79636300  11/20
决赛排名：/12
以下分享下自己在基于asir大佬（官方baseline）做的一些工作。

赛题链接：https://www.heywhale.com/home/competition/63eee2950644cee838881588/content/0

本方法的整体思路具体如下：

1. 数据读取与数据预处理
1.1 读取数据
对于训练数据中的食物特征集（train_food.csv）、疾病特征集（disease_feature1.csv、disease_feature2.csv、disease_feature3.csv）、食物疾病关系数据（train_answer.csv）、
食物与疾病关系的相关性评级（semi_train_answer.csv），进行直接读取。然后，将semi_train_answer.csv进行预处理，将related_score（取值为0，1，2，3，4）中取值为0返回0；
取值为1，2，3，4返回1，得到"is_related"字段，构成既包含"is_related"字段又包含"related_score"的训练数据（processing_semi_train_answer.csv）。


1.2 对food_id与disease_id进行编码
本方法通过直接采用food_id与disease_id中的数字对food_id与disease_id进行编码。

1.3 目标编码
由于数据集中只有两个离散变量food和disease，而测试集中都是新的food，所以用于目标编码的离散字段只有"disease"，本方法采用五折分层抽样的方法对标签"is_related"进行目标编码，
得到特征"disease_is_related_mean"。

1.4 疾病特征数据归一化、降维处理与缺失值填充
由于本方法通过PCA方法对疾病特征数据降维，因此有必要先对疾病特征数据归一化。首先，本方法采用用0填充降维后的疾病特征数据中的缺失值。其次，本方法通过MinMaxScaler()最大最小值的
归一化方法对疾病特征数据进行归一化。归一化完毕之后，采用PCA方法对疾病特征数据降维，分别将disease_feature1由996维降至128维，将disease_feature2由3181维降至144维，
将disease_feature3由1453维降至256维。

1.5 数据合并
经过上述数据预处理后，采用merge函数将疾病特征数据与食物特征数据进行合并。

2. 特征工程
2.1 重要特征做log1p变换
通过EDA分析，数据中很多特征存在长尾效应，会影响模型训练和泛化的效果。因此，根据模型输出的特征重要性，本方法采用np.log1p方法对部分重要的特征做log1p变换，去除长尾效应。

2.2 重要特征交叉
特征交叉可以将样本映射至高维空间，从而增加模型的非线性能力,提升模型的预测效果。因此，本方法对部分重要特征（与标签相关性强，具有区分能力的特征），
包含"N_33", "N_42", "N_74", "N_106", "N_111", "N_209", "disease", "food"，进行四则运算的特征交叉，提高模型训练和预测效果。

2.3 重要食物特征与疾病特征进行特征交叉
为了结合食物与疾病特征，提升模型效果，本方法对重要的食物特征"N_33"与重要的疾病特征"F_82_x", "F_39_x"进行特征交叉。

2.4 重要特征分箱处理
本方法通过EDA分析（绘制概率密度图）发现，部分重要特征在不同的取值上食物与疾病是否相关的概率不一样，并且这种关系是非线性的，且具有较强的区分能力。
因此本方法考虑对部分重要特征进行分箱处理，分箱离散化之后更能够刻画这种关系，且能够去噪，模型会更稳定，降低过拟合的风险。结果为：三类（0，1，2）分箱的特征：N_33, N_42, N_74, N_111；
四类（0，1，2，3）分箱的特征：N_106；10段：N_209和food；14段：disease。

2.5 统计特征构造
本方法通过相关性分析，对前面特征工程构造的特征研究其与标签"is_related"的相关性，从中选择具有较强相关性的特征按"food"分组进行统计特征构造，
分别构造“mean”、“std”、“max”、“min”这四种统计特征，从而提升特征的表达能力。

3. 特征筛选/选择
3.1 删除特征重要性较低的特征
为了避免不重要特征给模型训练带来负面影响，本方法根据特征重要性，删除特征重要性<100的特征。

3.2 删除单一取值的特征
单一取值的特征也会给模型训练带来负面影响，因此方法删除了只有单一取值的特征。

3.3 删除缺失率过高的特征
本方法通过设置阈值为0.95，删除了数据中缺失率大于等于0.95的特征。

4. 划分训练集测试集
根据"is_related"是否为空来划分训练集df_train和测试集df_test，在所有特征中去除 "disease_id", "food_id", "is_related", "related_score"，得到特征数据字段features_name，
特征训练数据为df_train[features_name]，测试集数据为df_test[features_name]，训练集标签为df_train["is_related"]。

5. 模型训练
本方法基于xgboost算法，通过10折交叉验证训练来训练模型（线上训练）。其中的参数通过optuna工具来自动调参，从而得到在当前特征数据下的最优参数：
{'max_depth': 8, 'subsample': 0.6538269745359124, 'min_child_weight': 1.3310020580145827, 'eta': 0.055429005058745304, 'gamma': 1.6102668438238998, 
'reg_alpha': 0.011767799429858665, 'reg_lambda': 1.790366108891338e-05, 'colsample_bytree': 0.5433494534450877, 'colsample_bylevel': 0.6918888106850691}。
模型训练之后可以得到训练集和测试集上的预测概率，分别为 xgb_oof和xgb_pred。

6. 测试集概率后处理
为了拟合线上阈值（0.5），本方法将模型对测试集预测的概率加0.36作为最终测试集的预测概率。另外，为了优化NDCG指标，本方法还做了额外的概率后处理操作： 
基于以上的特征工程得到训练特征数据，标签为related_score（取值为0，1，2，3，4），然后基于xgboost10折交叉验证训练训练一个多分类模型
（名称为“训练”的Notebook，采用训练数据训练模型，特征工程与上述一致，标签为related_score），并将每一折训练的模型以及ntree_limit保存下来，用于对测试集的related_score进行预测，
得到每一对food_id和disease_id的相关性评级。然后为每个评级（0，1，2，3，4）进行归一化处理，将评级映射到[0,1]之间，得到调整因子。
根据这个调整因子，使用指数函数来拟合概率与评级的关系，得到后处理之后的概率作为最终测试集概率结果，写入提交文件中。
